---
title: "Standardized Effect Size Simulation"
author: "Nicholas Knoblauch"
date: 2017-07-27
output: html_document
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}
```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```

<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
```{r code-version, echo=FALSE, results='asis'}
```

# Background


## The GWAS summary statistic

Under an additive model

$$ y= X \beta + \epsilon$$

Where $X$ is genotype ($n$ by $p$), $\beta$ is a vector (length $p$) of fixed effects and $\epsilon$ is noise/error.

Because in general $p >>n$, we can't directly estimate the distribution of $\beta_i$.  Instead we have univariate summary statistics:

$$ \hat{\beta_j} := (X_j^TX_j)^{-1}X_j^Ty $$

$$ \hat{\sigma_j^2} := (nX_j^TX_j)^{-1}(y-X_j\hat{\beta_j})^T(y-X_j\hat{\beta_j}) $$

## RSS

RSS relates univariate statistics to their multivariate counterparts by using the LD matrix:

$$ \hat{\beta} \sim N(\hat{S}\hat{R}\hat{S}^{-1},\hat{S}\hat{R}\hat{S}) $$

The original implementation of RSS uses a sparse prior on $\beta$, and involves lots of MCMC / variational inference to get a posterior on $\beta$

<!-- ### RSS with a normal prior -->

<!-- With a normal prior on $\beta$ ($\beta \sim N(0,\sigma_\beta)$), if we're only interested in $\sigma_\beta$, we can  -->

<!-- $$ \hat{\beta} | \beta \sim N(SRS^{-1}\beta,SRS) $$ -->







# Simulating from genotype

The main idea is that we have two parameters we want to estimate ($PVE,c$) from data $\hat{u}=\frac{\hat{\beta}}{\text{se}(\hat{\beta})}$

The path from $PVE$ and $c$  to $\hat{u}$ looks like this:

Start with an $n$x$p$ matrix of column-centered genotypes ($X$).

For a chosen value of $PVE$, define $\sigma_u$ as:

$$\sigma_u=\sqrt{\frac{n}{p}PVE}$$

$$u_i \sim N(0,\sigma_u)$$

$\beta$ is a transformation of $u$ based on $\sigma_y$ and $\sigma_{x_j}$ ($\sigma_y$ is chosen to be 1 for all simulations)


$$\beta_i=\frac{\sigma_y}{\sqrt{n}\sigma_{x_i}} u_i$$
From there we can construct $V(X\beta)$ which we can combine with our chosen PVE value to obtain the scale($\tau^{-1}$) of the residuals($\epsilon$):

$$ PVE= \frac{V(X\beta)}{\tau^{-1} + V(X\beta)} \implies \tau^{-1} = V(X\beta) \left(\frac{1}{PVE}-1\right)  $$
$$\epsilon \sim N(0,\tau^{-1}I_n)$$


$$ y= X \beta + \epsilon $$
$y$ is centered to have a mean of $0$. $\hat{\beta_i}$ and $\text{se}(\hat{\beta_i})$ Are obtained by univariate ordinary least squares (fit without an intercept term). If there is confounding in the simulation, it's added to $\hat{\beta}$ as $\hat{u}_{\text{confound}}=\hat{u}+N(0,c I_p)$


# Simulating "directly" from LD

A simpler simulation strategy is to simply sample $\hat{u}$ directly from a multivariate normal distribution, specified by $R$ $\sigma_u$, and $c$.


$$\hat{u}_{\text{confound}} \sim N(0,\sigma_u^2R^2+R+c I_p)$$

To (greatly) accelerate the generation of samples from the multivariate normal distribution, we can use the eigenvalue decomposition of $R$:

First remember that we can write the variance of $\hat{u}_{\text{confound}}$ as 

$$V(\sigma_u,c)=Q(\sigma^2_uD^2+D+c I_p)Q^{T}$$

for convenience, let's define $D^\star$ to be 

$$D^{\star}=(\sigma_uD^2+D+c I_p)$$
Let's also define $R^\star$ to be 
$$R^\star=Q D^{\star}Q^{T}$$

A useful trick when trying to draw samples from a multivariate normal distribution
is to use the matrix $A=QD^{1/2}$ and  draw $p$ samples from a standard normal distribution, (i.e $z_i \sim N(0,1)$).  $\hat{u}_{\text{confound}}=Az$ now has the desired distrubtion.

<!-- Add your analysis here -->

<!-- ## Session information -->

<!-- <!-- Insert the session information into the document --> -->
<!-- ```{r session-info} -->
<!-- ``` -->
