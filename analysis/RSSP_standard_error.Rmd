---
title: "Deriving the Standard Error for RSSp"
author: "Nicholas Knoblauch"
date: 2017-11-19
output: html_document
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}
```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```

<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
```{r code-version, echo=FALSE, results='asis'}
```


#Background 

## Fisher's Approximation in the  i.i.d case

If we have $n$ independent data points, each with the distribution $f(x|\theta)$, for large $n$, the MLE $\hat{\theta}$ as approximately normal, with mean $\theta$, and variance $\frac{\tau^2(\theta)}{n}$, where 

$$ \frac{1}{\tau^2(\theta)}=E \left( \frac{d}{d \theta} \log f(X_1|\theta) \right)^2 = -E \left[ \frac{d^2}{d\theta^2} \log f(X_1|\theta) \right]$$

## What if data is independent but not identical?

Consider the scenario where $X_i$ has density $f_i(x|\theta)$, and 

$$\sigma^2_i(\theta) = E \left( \frac{d}{d\theta} \log f_i(X_i|\theta) \right)^2 = -E \left[ \frac{d}{d\theta^2} \log f_i(X_i|\theta) \right]$$

In this case, $\sqrt{n}(\hat{\theta}-\theta)$ is approximately normal with an expectation of $0$  and a variance given by $$\frac{1}{\sum_{i=1}^n \sigma_i^2(\theta)}$$.  (This result comes from equation 5.77 of the text of Stigler's STAT 244 class)

## The RSSp scenario

Remember the marginalized form of $\hat{u}$ (or check out the `RSSp_Posterior` post)

$$ \hat{\textbf{u}}|\sigma_u^2,c \sim N(0,\sigma_u^2R^2+R+cI_p)$$
Also remember that we have diagonalized the LD matrix:

$$\sigma^2_uR^2+R+cI_p \\ = \sigma_u^2QD_R^2Q^{T} + Q D_{R} Q^{T} + cI_p \\ =\sigma_u^2QD_R^2Q^{T}+QD_LQ^{T} \\ =Q(\sigma_u^2D^2_R + D_L)Q^{T} \\ =Q(D_\textbf{u})Q^{T}$$
Where $D_R=\text{diag}\left(\lambda_i\right)$ ,$D_L=\text{diag}\left(\lambda_i+c\right)$ and $D_\textbf{u}=\text{diag}\left(\sigma_u^2\lambda_i^2+\lambda_i+c\right)$

If we transform $\hat{\textbf{u}}$, multiplying it by $Q^{T}$, then instead of having a multivariate  $\hat{\textbf{u}}|\sigma_u^2,c$ , we now have $p$ univariate normals, with densities given by 

$$(Q^{T}\hat{\textbf{u}})_i|\sigma_u^2,c  \sim N(0,\sigma_u^2\lambda_i^2+\lambda_i+c)$$

If we call $(Q^{T}\hat{\textbf{u}})_i$ $\hat{q}_i$ then we can write the log-likelihood as:

$$-\frac{1}{2} \frac{\hat{q}_i^2}{c + \lambda_i^2 \sigma_u^2 + \lambda_i} - \frac{1}{2} \log(c + \lambda_i^2 \sigma_u^2 + \lambda_i) + \frac{1}{2} (-\log(2) - \log(Ï€))$$
The first derivative wrt. $\sigma_u^2$ is

$$-\frac{\lambda_i^2 (c + \lambda_i^2 \sigma_u^2 + \lambda_i - \hat{q}_i^2)}{2 (c + \lambda^2 \sigma_u^2 + \lambda_i)^2}$$

The second derivative wrt. $\sigma_u^2$ is :

$$\frac{\lambda_i^4 (c + \lambda_i^2 \sigma_u^2 + \lambda_i - 2 \hat{q}_i^2)}{2 (c + \lambda_i^2 \sigma_u^2 + \lambda_i)^3}$$

The first derivative wrt. $c$ is

$$ -\frac{c + \lambda_i^2 \sigma_u^2 + \lambda_i - \hat{q}_i^2}{2 (c + \lambda_i^2 \sigma_u^2 + \lambda_i)^2}$$
The second derivative wrt. $c$ is

$$\frac{c   + \lambda_i^2 \sigma_u^2 + \lambda_i - 2 \hat{q}_i^2}{2 (c  + \lambda_i^2 \sigma_u^2+ \lambda_i)^3}$$

Finally, the cross term is:
 $$\frac{\lambda_i^2 (c + \lambda_i^2 \sigma_u^2 + \lambda_i - 2 \hat{q}_i^2)}{2 (c + \lambda_i^2 \sigma_u^2 + \lambda_i)^3}$$
If  we define $\theta = \left\{ \sigma_u^2 , c \right\}$, and $H_{.,.,i}$ to be the symmetric 2x2 Hessian matrix:

$$H_{.,.,i}=\begin{bmatrix}\frac{\lambda_i^4 (c + \lambda_i^2 \sigma_u^2 + \lambda_i - 2 \hat{q}_i^2)}{2 (c + \lambda_i^2 \sigma_u^2 + \lambda_i)^3} & \frac{\lambda_i^2 (c + \lambda_i^2 \sigma_u^2 + \lambda_i - 2 \hat{q}_i^2)}{2 (c + \lambda_i^2 \sigma_u^2 + \lambda_i)^3}\\\frac{\lambda_i^2 (c + \lambda_i^2 \sigma_u^2 + \lambda_i - 2 \hat{q}_i^2)}{2 (c + \lambda_i^2 \sigma_u^2 + \lambda_i)^3} & \frac{c + \lambda_i^2 \sigma_u^2+ \lambda_i - 2 \hat{q}_i^2 }{2 (c  + \lambda_i^2 \sigma_u^2+ \lambda_i)^3}\end{bmatrix} =H_{.,.,i}=\frac{c + \lambda_i^2 \sigma_u^2 + \lambda_i - 2 \hat{q}_i^2}{2 (c + \lambda_i^2 \sigma_u^2 + \lambda_i)^3}   
\begin{bmatrix} \lambda_i^4 & \lambda_i^2\\ \lambda_i^2 & 1\end{bmatrix} \\
=\frac{c + \lambda_i^2 \sigma_u^2 + \lambda_i - 2 \hat{q}_i^2}{2 (c + \lambda_i^2 \sigma_u^2 + \lambda_i)^3} \begin{bmatrix}\lambda_i^2 \\ 1 \end{bmatrix} \begin{bmatrix}\lambda_i^2 & 1 \end{bmatrix}$$
Then

$$\sigma^2_i(\theta_j) = E \left( \frac{d}{d\theta_j} \log f_i(X_i|\theta) \right)^2 = H^{-1}_{j,j,i}$$



This means that 
In this case, $\sqrt{p}(\hat{\theta}-\theta)$ is approximately normal with an expectation of $0$  and a variance given by  $$\left(\sum_{i=1}^p \sigma_i^2(\theta)\right)^{-1}=\left(\sum_{i=1}^p - \frac{c + \lambda_i^2 \sigma_u^2 + \lambda_i - 2 \hat{q}_i^2}{2 (c + \lambda_i^2 \sigma_u^2 + \lambda_i)^3}   
\begin{bmatrix} \lambda_i^4 & \lambda_i^2\\ \lambda_i^2 & 1\end{bmatrix}\right)^{-1}$$



Note that the case of mutually independent SNPs (i.e $R=I_p$). 

$$H^{-1}=\left(\sum_{i=1}^p - \frac{c +  \sigma_u^2 + 1 - 2 \hat{q}_i^2}{2 (c + \sigma_u^2 + 1)^3}   
\begin{bmatrix} 1 & 1\\ 1 & 1\end{bmatrix}\right)^{-1}=\sum_{i=1}^p - \frac{2 (c + \sigma_u^2 + 1)^3}{c +  \sigma_u^2 + 1 - 2 \hat{q}_i^2}
\left(\begin{bmatrix} 1 & 1\\ 1 & 1\end{bmatrix}\right)^{-1}$$

The matrix $\begin{bmatrix} 1 & 1\\ 1 & 1\end{bmatrix}$ is singular, as are all constant multiples of this matrix.  This is perhaps not surprising given that in the case that all SNPs are unlinked, variance arising from $\sigma_u^2$ and $c$ are entirely indistinguishable.  This is born out in simulation:


## Simulation


To verify the degenerate behavior in the independent SNP setting, we set $p=100$,$R=I_p$, $\sigma_u=1$ and $c=0.2$.  The singularity of the Information Matrix will be assessed by computing the determinant of the hessian matrix returned by `optim` (these results were also verified by computing the hessian using it's analytical form)
```{r R_identity, echo=F,message=F,warning=F}

library(tidyverse)
library(RSSp)
p <- 100
R <- diag(p)
reps <- 200
sigma_u <- 1
evdR <- eigen(R)
evdQ <- evdR$vectors
D <- evdR$values
a <- 0.2
u_data <- rnorm(p,mean=0,sd=sigma_u)
# u_hat <- t(mvtnorm::rmvnorm(n=reps,mean=R%*%u_data,sigma=R)) %>%   array_branch(2) %>%
#   imap_dfr(~data_frame(rep=.y,beta_hat=.x,beta=u_data))

u_hat <- mvtnorm::rmvnorm(n=reps,mean=R%*%u_data,sigma=R+a*diag(p)) %>%   array_branch(1) %>%
  imap_dfr(~data_frame(fgeneid=as.character(.y),beta_hat=.x,beta=u_data,quh=c(crossprod(evdQ,beta_hat)),D=D,p_n=.5))

rssp_est <- group_by(u_hat,fgeneid) %>% do(RSSp_estimate(.,doConfound=T)) %>% ungroup() %>% arrange(as.integer(fgeneid))
``` 



```{r}
ggplot(rssp_est,aes(x=fisher_det))+geom_histogram()+xlab(bquote(group("|",H,"|")))+ggtitle("The Hessian is singular in the absence of LD")
```


Now let's add a non-diagonal LD matrix
```{r R_sim}
p <- 100
n <- p*10
R <- cor(matrix(rnorm(n*p),n,p))
evdR <- eigen(R)
a <- 0.2
evdQ <- evdR$vectors
D <- evdR$values
reps <- 500
sigma_u <- 1


u_data <- rnorm(p,mean=0,sd=sigma_u)
u_hat <- mvtnorm::rmvnorm(n=reps,mean=R%*%u_data,sigma=R+a*diag(p)) %>%   array_branch(1) %>%
  imap_dfr(~data_frame(fgeneid=as.character(.y),beta_hat=.x,beta=u_data,quh=c(crossprod(evdQ,beta_hat)),D=D,p_n=.5))
```


```{r}
rssp_est <- group_by(u_hat,fgeneid) %>% do(RSSp_estimate(.,doConfound=T)) %>% ungroup() %>% arrange(as.integer(fgeneid))

```




```{r}
ggplot(rssp_est,aes(x=fisher_det))+geom_histogram(bins=50)+xlab(bquote(group("|",H,"|")))+ggtitle("The Hessian is non-singular in the presence of LD")
```




Are our estimates centered around the true value? How well does the asymptotic normality assumption work with ~100 SNPs?


In this plot, each point is the MLE of $\sigma_u^2,c$ from a random draw from $\hat{\textbf{u}}|\textbf{u},c,R$.  The vertical bars represent the asymptotic estimate of variance in $c$, the horizontal bars represent the asymptotic estimate of  variance in $\sigma_u^2$, and the diagonal bar represents the estimate of covariance between $c$ and $\sigma_u^2$.  The gold colored lines indicate the true value of $\sigma_u^2$ and $c$

```{r}
filter(rssp_est,abs(sigu_bias_cov)<1) %>% 
ggplot(aes(x=sigu^2,y=bias))+geom_point()+geom_errorbar(aes(ymin=(bias-bias_var),ymax=(bias+bias_var))) +
  geom_errorbarh(aes(xmin=sigu^2-sigu_var,xmax=sigu^2+sigu_var)) +
  geom_spoke(aes(angle=pi/4,radius=sigu_bias_cov)) +
  geom_hline(yintercept = a,col="gold",size=2,alpha=0.5) +
  geom_vline(xintercept=sigma_u^2,col="gold",size=2,alpha=0.5)+xlab(bquote(hat(sigma)[u]^2))+ylab(bquote(hat(c)))
```



Here we see that the estimated variance of the confounding parameter is highly correlated to the estimated variance of $\sigma^2_\textbf{u}$


```{r}
filter(rssp_est,sigu_var<15) %>% ggplot(aes(x=sigu_var,y=bias_var))+geom_point()+xlab(bquote(Var(hat(sigma)[u]^2)))+ylab(bquote(Var(hat(c))))+geom_abline(slope=1,intercept=0)+geom_smooth(method="lm")

```





<!-- <!-- Insert the session information into the document -->
<!-- ```{r session-info} -->
<!-- ``` -->
